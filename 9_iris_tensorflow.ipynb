{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9 iris tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk98RinhKyQn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "outputId": "89020483-affc-4a9b-ae99-a5d44732908e"
      },
      "source": [
        "#imports\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "#feature column names\n",
        "feature_column_names = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
        "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
        "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
        "\n",
        "#bring in the data...\n",
        "train_dataset = pd.read_csv(filepath_or_buffer=\"/content/iris_training.csv\", names=CSV_COLUMN_NAMES, header=0)\n",
        "test_dataset = pd.read_csv(filepath_or_buffer=\"/content/iris_test.csv\", names=CSV_COLUMN_NAMES, header=0)\n",
        "\n",
        "#split up the training data and define as features/labels\n",
        "train_features = train_dataset.drop(columns=train_dataset.columns[4], axis = 1)\n",
        "train_labels = train_dataset.drop(columns=train_dataset.columns[[0,1,2,3]], axis = 1)\n",
        "\n",
        "#split up the testing data and define as features/labels\n",
        "test_features = test_dataset.drop(columns=test_dataset.columns[4], axis = 1)\n",
        "test_labels = test_dataset.drop(columns=test_dataset.columns[[0,1,2,3]], axis = 1)\n",
        "\n",
        "# Feature columns describe how to use the input.\n",
        "my_feature_columns = []\n",
        "\n",
        "my_feature_columns.append(tf.feature_column.numeric_column(key=\"SepalLength\"))\n",
        "my_feature_columns.append(tf.feature_column.numeric_column(key=\"SepalWidth\"))\n",
        "my_feature_columns.append(tf.feature_column.numeric_column(key=\"PetalLength\"))\n",
        "my_feature_columns.append(tf.feature_column.numeric_column(key=\"PetalWidth\"))\n",
        "\n",
        "#initialize classifier\n",
        "classifier = tf.estimator.DNNClassifier\\\n",
        "(\n",
        "    feature_columns = my_feature_columns,\n",
        "    hidden_units = [10, 10],\n",
        "    n_classes = 3\n",
        ")\n",
        "\n",
        "#define the TRAINING input function\n",
        "def input_fn_train(features, labels, batch_size):\n",
        "    #convert features/labels to dataset object\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "    # Shuffle, repeat, and batch the examples.\n",
        "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
        "\n",
        "    #return dataset\n",
        "    return dataset\n",
        "\n",
        "#train the DNN\n",
        "classifier.train\\\n",
        "(\n",
        "    input_fn = lambda : input_fn_train(features=train_features, labels=train_labels, batch_size=100),\n",
        "    steps = 100\n",
        ")\n",
        "\n",
        "\n",
        "print('fit done')\n",
        "\n",
        "#define the TESTING input function\n",
        "def input_fn_test(features, labels, batch_size):\n",
        "    #convert features/labels to dataset object\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "    # Shuffle, repeat, and batch the examples.\n",
        "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
        "\n",
        "    #return dataset\n",
        "    return dataset\n",
        "\n",
        "eval_result = classifier.evaluate\\\n",
        "(\n",
        "    input_fn= lambda : input_fn_test(features=test_features, labels=test_labels, batch_size=10),\n",
        "    steps=100\n",
        ")\n",
        "\n",
        "print(eval_result)\n",
        "\n",
        "predict_x = \\\n",
        "    {\n",
        "        'SepalLength': [5.1, 5.9, 6.9],\n",
        "        'SepalWidth': [3.3, 3.0, 3.1],\n",
        "        'PetalLength': [1.7, 4.2, 5.4],\n",
        "        'PetalWidth': [0.5, 1.5, 2.1],\n",
        "    }\n",
        "\n",
        "def eval_input_fn(features, labels, batch_size):\n",
        "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
        "    features=dict(features)\n",
        "    if labels is None:\n",
        "        # No labels, use only features.\n",
        "        inputs = features\n",
        "    else:\n",
        "        inputs = (features, labels)\n",
        "\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
        "\n",
        "    # Batch the examples\n",
        "    assert batch_size is not None, \"batch_size must not be None\"\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    # Return the dataset.\n",
        "    return dataset\n",
        "\n",
        "\n",
        "predictions = classifier.predict \\\n",
        "(\n",
        "    input_fn = lambda : eval_input_fn(features = predict_x, labels = None, batch_size = 100)\n",
        ")\n",
        "\n",
        "# Generate predictions from the model\n",
        "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
        "\n",
        "template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
        "\n",
        "for pred_dict, expec in zip(predictions, expected):\n",
        "    class_id = pred_dict['class_ids'][0]\n",
        "    probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "    print(template.format(SPECIES[class_id], 100 * probability, expec))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp13nqk4r1\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp13nqk4r1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8e3ac57898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp13nqk4r1/model.ckpt.\n",
            "INFO:tensorflow:loss = 287.61252, step = 1\n",
            "INFO:tensorflow:Saving checkpoints for 100 into /tmp/tmp13nqk4r1/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 14.579449.\n",
            "fit done\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-01-09T22:03:04Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmp13nqk4r1/model.ckpt-100\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Evaluation [50/100]\n",
            "INFO:tensorflow:Evaluation [60/100]\n",
            "INFO:tensorflow:Evaluation [70/100]\n",
            "INFO:tensorflow:Evaluation [80/100]\n",
            "INFO:tensorflow:Evaluation [90/100]\n",
            "INFO:tensorflow:Evaluation [100/100]\n",
            "INFO:tensorflow:Finished evaluation at 2020-01-09-22:03:04\n",
            "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.966, average_loss = 0.14096242, global_step = 100, loss = 1.4096242\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: /tmp/tmp13nqk4r1/model.ckpt-100\n",
            "{'accuracy': 0.966, 'average_loss': 0.14096242, 'loss': 1.4096242, 'global_step': 100}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmp13nqk4r1/model.ckpt-100\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "\n",
            "Prediction is \"Setosa\" (97.8%), expected \"Setosa\"\n",
            "\n",
            "Prediction is \"Versicolor\" (91.0%), expected \"Versicolor\"\n",
            "\n",
            "Prediction is \"Virginica\" (63.5%), expected \"Virginica\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJRmAOZ2TTLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}